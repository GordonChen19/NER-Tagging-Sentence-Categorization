{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsCGGYu-JLkt",
        "outputId": "6f405a2a-b891-447b-d68d-7389cb8a14e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhncx65Ido0y",
        "outputId": "c9e47455-9b26-4ab8-81b2-c33d48bf19c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "import os\n",
        "\n",
        "os.chdir(\"..\")"
      ],
      "metadata": {
        "id": "9OlIhoW4vrEz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UxIrzzxxCu-",
        "outputId": "aa106c18-9f29-48cc-fbeb-5a6f9e3c0ed2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query the most similar vector\n"
      ],
      "metadata": {
        "id": "Qe580KAqz4f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"student\", \"Apple\", \"apple\"]\n",
        "\n",
        "for w in words:\n",
        "    most_sim_w, score = w2v.most_similar(positive= [w])[0]\n",
        "    print(f\"{w}:  {most_sim_w}, {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8egMwJi7q7V",
        "outputId": "1ebf29b6-6e3e-47db-d80c-f85dccf6cec7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "student:  students, 0.729\n",
            "Apple:  Apple_AAPL, 0.746\n",
            "apple:  apples, 0.720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Donwload dataset"
      ],
      "metadata": {
        "id": "2244o3tEF9rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chBpBd-DGA5D",
        "outputId": "f7cb5740-35e9-4379-d72b-b0574c622795"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=405cf9a72b1acc3c676c3215f78556c6a25ad3c40c6ccb4daa5947e00eab7dfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget"
      ],
      "metadata": {
        "id": "Y7NSvzgxHEso"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1.2 Download\n",
        "\n",
        "conll_raw_url = \"https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/\"\n",
        "filenames = [\"eng.train\", \"eng.testa\", \"eng.testb\"]\n",
        "\n",
        "urls = {(f, f\"{conll_raw_url}/{f}\") for f in filenames}\n",
        "\n",
        "for fn, url in urls:\n",
        "    save_path = f\"/content/drive/MyDrive/NLPdataset/{fn}\"\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"{fn} already exists. Skipping\")\n",
        "        continue\n",
        "    wget.download(url, save_path)"
      ],
      "metadata": {
        "id": "JYGBOxg_HGTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed723b8-ae0c-4d31-d8e5-d31288ebebf1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eng.train already exists. Skipping\n",
            "eng.testb already exists. Skipping\n",
            "eng.testa already exists. Skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition Dataset into training, development and test sets"
      ],
      "metadata": {
        "id": "a75ajFCmeRsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File has one line\n",
        "\n",
        "# First column is the word\n",
        "# Second is POS tag\n",
        "# Third is Consistuency parsing tag\n",
        "# Fourth is NER tag\n",
        "\n",
        "# The NER tagging column\n",
        "\n",
        "\n",
        "root = '/content/drive/MyDrive/NLPdataset/'\n",
        "\n",
        "# Returns a 3 dim array of sentences x words x (word_value, word_category)\n",
        "def process_sets(filepath):\n",
        "    raw = open(filepath)\n",
        "    fin, curr = [], []\n",
        "    tags = []\n",
        "\n",
        "    for r in raw:\n",
        "        if r == \"\\n\":\n",
        "            fin.append(curr)\n",
        "            curr = []\n",
        "            continue\n",
        "        elif 'DOCSTART' in r:# Some files have these which are used to divide sentences\n",
        "          continue\n",
        "        else:\n",
        "          r = r[:-1].split()\n",
        "\n",
        "        if(len(r)!=4):\n",
        "          print(r)\n",
        "        curr.append([r[0],r[3]]) #Extract first and last column (NER Tag)\n",
        "        if r[3] not in tags:\n",
        "          tags.append(r[3])\n",
        "\n",
        "\n",
        "    fin.append(curr)\n",
        "    return fin,tags\n",
        "\n",
        "trainset,traintags = process_sets(root+ \"eng.train\")\n",
        "devset, devtags= process_sets(root +\"eng.testa\")   # aka validation set\n",
        "testset, testtags = process_sets(root +\"eng.testb\")\n",
        "\n",
        "print (f'trainset (No. sentences): {len(trainset)}')\n",
        "print (f'developmentset (No. sentences): {len(devset)}')\n",
        "print (f'testset (No. sentences): {len(testset)}')\n",
        "\n",
        "print(f'tags in training set are {traintags}')\n",
        "print(f'tags in development set are {devtags}')\n",
        "print(f'tags in testing set are {testtags}')\n",
        "print(f'complete set of tags: {set(traintags+devtags+testtags)}')"
      ],
      "metadata": {
        "id": "3qmxfDnyJKrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802b4e44-41b1-4289-808b-3678723d4b57"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainset (No. sentences): 14987\n",
            "developmentset (No. sentences): 3466\n",
            "testset (No. sentences): 3684\n",
            "tags in training set are ['I-ORG', 'O', 'I-MISC', 'I-PER', 'I-LOC', 'B-LOC', 'B-MISC', 'B-ORG']\n",
            "tags in development set are ['O', 'I-ORG', 'I-LOC', 'I-MISC', 'I-PER', 'B-MISC']\n",
            "tags in testing set are ['O', 'I-LOC', 'I-PER', 'I-MISC', 'I-ORG', 'B-ORG', 'B-MISC', 'B-LOC']\n",
            "complete set of tags: {'I-MISC', 'I-LOC', 'O', 'B-MISC', 'B-ORG', 'I-PER', 'B-LOC', 'I-ORG'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example sentence"
      ],
      "metadata": {
        "id": "YbqvO-KcfF2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def joinString(entity, joinentity):\n",
        "    if(entity==\"\"):\n",
        "        return joinentity\n",
        "    elif(joinentity[0]==\"'\"):\n",
        "        return entity + joinentity\n",
        "\n",
        "    else:\n",
        "        return entity + \" \" + joinentity\n",
        "\n",
        "\n",
        "def getSentence():\n",
        "    for sentence in trainset:\n",
        "        named_entities = []\n",
        "        previous_word = ''\n",
        "        counter = 0 #Number of entities with more than one word\n",
        "        current_tag = None #B,I,O\n",
        "        entity_tag = None #e.g. PER, LOC, MISC\n",
        "        word_length = 0\n",
        "        for i,word in enumerate(sentence):\n",
        "            if word[1]=='O': #If this is not an entity\n",
        "                current_tag=None\n",
        "                if word_length>1:\n",
        "                    counter+=1\n",
        "                if previous_word!='':\n",
        "                    named_entities.append(previous_word)\n",
        "                previous_word=''\n",
        "                word_length=0\n",
        "            else:\n",
        "                if current_tag is None: #If the current named entity succeeds a 'O' or is the first word of the sentence\n",
        "                    current_tag,entity_tag = word[1].split('-')\n",
        "                    previous_word=joinString(previous_word,word[0])\n",
        "                    word_length=1\n",
        "\n",
        "                else:\n",
        "                    tmp_tag,tmp_type = word[1].split('-')\n",
        "                    if tmp_tag == 'B' or tmp_type!=entity_tag: #Beginning of a new named entity\n",
        "                        if word_length>1:\n",
        "                            counter+=1\n",
        "                        named_entities.append(previous_word)\n",
        "                        previous_word=''\n",
        "                        word_length=0\n",
        "                        current_tag,entitiy_tag=word[1].split('-')\n",
        "                    else: #Continuation of the previous entity\n",
        "                        previous_word=joinString(previous_word,word[0])\n",
        "                        current_tag=tmp_tag\n",
        "                        word_length+=1\n",
        "\n",
        "                    if i == len(sentence)-1 and previous_word!='':\n",
        "                        if(word_length>1):\n",
        "                            counter+=1\n",
        "                        named_entities.append(previous_word)\n",
        "\n",
        "        if counter>2:\n",
        "            return named_entities,sentence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nO-ov1NI2kg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities, sentence_tags = getSentence()\n",
        "\n",
        "sentenceString = \"\"\n",
        "for word in sentence_tags:\n",
        "    sentenceString=joinString(sentenceString,word[0])\n",
        "\n",
        "print(entities)\n",
        "print(sentenceString)\n",
        "print(sentence_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK6zrjsbg_lb",
        "outputId": "3155ad87-4f13-47b1-d2d4-ea4d62346bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Germany', \"Welsh National Farmers' Union\", 'NFU', 'John Lloyd Jones', 'BBC radio']\n",
            "\" What we have to be extremely careful of is how other countries are going to take Germany's lead , \" Welsh National Farmers' Union ( NFU ) chairman John Lloyd Jones said on BBC radio .\n",
            "[['\"', 'O'], ['What', 'O'], ['we', 'O'], ['have', 'O'], ['to', 'O'], ['be', 'O'], ['extremely', 'O'], ['careful', 'O'], ['of', 'O'], ['is', 'O'], ['how', 'O'], ['other', 'O'], ['countries', 'O'], ['are', 'O'], ['going', 'O'], ['to', 'O'], ['take', 'O'], ['Germany', 'I-LOC'], [\"'s\", 'O'], ['lead', 'O'], [',', 'O'], ['\"', 'O'], ['Welsh', 'I-ORG'], ['National', 'I-ORG'], ['Farmers', 'I-ORG'], [\"'\", 'I-ORG'], ['Union', 'I-ORG'], ['(', 'O'], ['NFU', 'I-ORG'], [')', 'O'], ['chairman', 'O'], ['John', 'I-PER'], ['Lloyd', 'I-PER'], ['Jones', 'I-PER'], ['said', 'O'], ['on', 'O'], ['BBC', 'I-ORG'], ['radio', 'I-ORG'], ['.', 'O']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out of vocabulary word preprocessing\n"
      ],
      "metadata": {
        "id": "kmWItuCWYDWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get all words not in the word2vec model\n",
        "def get_words_not_in_model(set, w2v):\n",
        "    abs_w2v = [] # array to store words that are not in the word2vec model\n",
        "    abs_w2v_lower = [] # array to store words that are not in the word2vec model, but are in lower case\n",
        "    for sentences in set:\n",
        "        for word, type in sentences:\n",
        "            if word not in w2v.key_to_index:\n",
        "                # print(word)\n",
        "                abs_w2v.append(word)\n",
        "                if word.lower() not in w2v.key_to_index:\n",
        "                    abs_w2v_lower.append(word.lower())\n",
        "    return abs_w2v, abs_w2v_lower"
      ],
      "metadata": {
        "id": "QH51CeCQbYae"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# function to remove words that are puncutation, numbers, or special characters\n",
        "def remove_punc_num_special(s):\n",
        "    clean_set = []\n",
        "    for sentence in s:\n",
        "        clean_sentence = []\n",
        "        for word, tag in sentence:\n",
        "            if any(c.isalpha() for c in word):\n",
        "                clean_sentence.append([word, tag])\n",
        "        clean_set.append(clean_sentence)\n",
        "    return clean_set\n"
      ],
      "metadata": {
        "id": "8_QvwFbfbcr5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_stemming(s,w2v):\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_sentences=[]\n",
        "    for sentence in s:\n",
        "        stemmed_sentence=[]\n",
        "        for word, tag in sentence:\n",
        "            if (word not in w2v.key_to_index and  ps.stem(word) in w2v.key_to_index):\n",
        "                stemmed_sentence.append([ps.stem(word),tag])\n",
        "            else:\n",
        "                stemmed_sentence.append([word,tag])\n",
        "        stemmed_sentences.append(stemmed_sentence)\n",
        "    return stemmed_sentences"
      ],
      "metadata": {
        "id": "uynylU3_mn1L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(s):\n",
        "    stop_words=['of','a','and','to'] #'s\n",
        "    stopless_sentences=[]\n",
        "    for sentence in s:\n",
        "        stopless_sentence=[]\n",
        "        for word,tag in sentence:\n",
        "            if word not in stop_words:\n",
        "                stopless_sentence.append([word,tag])\n",
        "\n",
        "        stopless_sentences.append(stopless_sentence)\n",
        "    return stopless_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "Dd7zWWCEtNrd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyphen_seperation(s,w2v):\n",
        "    import re\n",
        "    # Define the regular expression pattern\n",
        "    pattern1 = r'([A-Za-z]+)-([A-Za-z]+)'\n",
        "    pattern2 = r'\\d{1,2}(?:st|nd|rd|th)'\n",
        "    pattern3 = r'([0-9]+)-([A-Za-z]+)'\n",
        "\n",
        "    seperated_sentences=[]\n",
        "    for sentence in s:\n",
        "        seperated_sentence=[]\n",
        "        for word,tag in sentence:\n",
        "            if re.match(pattern3,word):\n",
        "                left,right = word.split('-',1)\n",
        "                if(right in w2v.key_to_index):\n",
        "                    seperated_sentence.append([right,tag])\n",
        "                else:\n",
        "                    seperated_sentence.append([word,tag])\n",
        "            elif re.match(pattern2,word):\n",
        "                continue\n",
        "            elif re.match(pattern1,word):\n",
        "                left,right= word.split('-',1)\n",
        "                if(left in w2v.key_to_index and right in w2v.key_to_index):\n",
        "                    # print(word)\n",
        "                    seperated_sentence.append([left,tag])\n",
        "                    seperated_sentence.append([right,tag])\n",
        "                else:\n",
        "                    seperated_sentence.append([word,tag])\n",
        "            else:\n",
        "                seperated_sentence.append([word,tag])\n",
        "        seperated_sentences.append(seperated_sentence)\n",
        "    return seperated_sentences"
      ],
      "metadata": {
        "id": "p5NnFNhxQtIR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check number of words not in word2vec model\n",
        "fullset = trainset + devset + testset\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(fullset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU04NuJbctPA",
        "outputId": "d3f5e56a-6b2e-421d-e523-4e2e0c7c6191"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83402 83322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map all words to embeddings"
      ],
      "metadata": {
        "id": "HS065tylI-VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "dQI1m9BFOFST"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v['<UNK>']=np.zeros(300)\n",
        "\n",
        "word_to_ix = {\"<PAD>\":3000000,\"<UNK>\":3000000}\n",
        "tag_to_ix = {\"<PAD>\": 0, \"O\": 1, \"B-MISC\": 2, \"B-LOC\": 3, \"I-MISC\": 4, \"I-LOC\": 5, \"B-ORG\": 6, \"I-PER\": 7, \"I-ORG\": 8}\n",
        "#Map each word to a metric embedding\n",
        "for sentence in fullset:\n",
        "    for word,tag in sentence:\n",
        "        if word in w2v.key_to_index:\n",
        "            word_to_ix[word]=w2v.key_to_index[word]\n",
        "        elif word.lower() in w2v.key_to_index:\n",
        "            word_to_ix[word]=w2v.key_to_index[word.lower()]\n",
        "        else:\n",
        "            word_to_ix[word]=w2v.key_to_index['<UNK>']\n",
        ""
      ],
      "metadata": {
        "id": "0uPa_Y1wlJBm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46Bm3dlYO4Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading"
      ],
      "metadata": {
        "id": "w8K37mrRO4l3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LpPTYNJzO631"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import string\n"
      ],
      "metadata": {
        "id": "D-iubbQhO68G"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes in a set of sentences in the form of a list of lists of tuples, and\n",
        "# returns a list of list of words and tags\n",
        "def separate_words_tags(set):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for sentence in set:\n",
        "        current_sentence = []\n",
        "        current_label = []\n",
        "        for word, tag in sentence:\n",
        "            current_sentence.append(word)\n",
        "            current_label.append(tag)\n",
        "        sentences.append(current_sentence)\n",
        "        labels.append(current_label)\n",
        "    return sentences, labels\n",
        "\n",
        "# class to create a dataset for the NER task\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.sentences[index]\n",
        "        label = self.labels[index]\n",
        "        original_length = len(sentence)\n",
        "\n",
        "        sentence = [self.word_to_ix.get(word, 3000000) for word in sentence] # 3000000 is the index for <UNK>\n",
        "        label = [self.tag_to_ix[tag] for tag in label]\n",
        "\n",
        "        return torch.tensor(sentence, dtype=torch.long), torch.tensor(label, dtype=torch.long), original_length\n",
        "\n",
        "\n",
        "# function to pad the sequences in a batch\n",
        "def pad_collate(batch):\n",
        "    (xx, yy, lens) = zip(*batch)  # unzip the batch\n",
        "\n",
        "    x_lens = [len(x) for x in xx]  # get lengths of sequences\n",
        "    y_lens = [len(y) for y in yy]  # get lengths of labels\n",
        "\n",
        "    max_x_len = max(x_lens)\n",
        "    max_y_len = max(y_lens)\n",
        "\n",
        "    xx_pad = torch.full((len(xx), max_x_len), 3000000, dtype=torch.long)  # create a matrix filled with 3000000\n",
        "    yy_pad = torch.zeros(len(yy), max_y_len, dtype=torch.long)  # create a matrix of zeros with correct dimensions\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(xx, yy)):\n",
        "        xx_pad[i, :x_lens[i]] = x\n",
        "        yy_pad[i, :y_lens[i]] = y\n",
        "\n",
        "    return xx_pad, yy_pad, lens\n"
      ],
      "metadata": {
        "id": "1XuOB4rLEAyV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean dataset by removing meaningless words\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "fullset = remove_punc_num_special(fullset) #Remove punctuation, special characters and numbers\n",
        "fullset = word_stemming(fullset,w2v) #Porter Stemmer\n",
        "fullset = remove_stop_words(fullset) #Remove stop words\n",
        "fullset = hyphen_seperation(fullset,w2v) #Address hyphen separated words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(fullset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "#Trainset Transformation\n",
        "trainset = remove_punc_num_special(trainset) #Remove punctuation, special characters and numbers\n",
        "trainset = word_stemming(trainset,w2v) #Porter Stemmer\n",
        "trainset = remove_stop_words(trainset) #Remove stop words\n",
        "trainset = hyphen_seperation(trainset,w2v) #Address hyphen separated words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(trainset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "#Validation set transformation\n",
        "devset = remove_punc_num_special(devset) #Remove punctuation, special characters and numbers\n",
        "devset = word_stemming(devset,w2v) #Porter Stemmer\n",
        "devset = remove_stop_words(devset) #Remove stop words\n",
        "devset = hyphen_seperation(devset,w2v) #Address hyphen separated words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(devset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "#Test set transformation\n",
        "testset = remove_punc_num_special(testset) #Remove punctuation, special characters and numbers\n",
        "testset = word_stemming(testset,w2v) #Porter Stemmer\n",
        "testset = remove_stop_words(testset) #Remove stop words\n",
        "testset = hyphen_seperation(testset,w2v) #Address hyphen separated words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(testset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "\n",
        "#train\n",
        "\n",
        "sentences, labels = separate_words_tags(trainset) # separate words and tags\n",
        "train_data = NERDataset(sentences, labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "sentences, labels = separate_words_tags(devset)\n",
        "dev_data = NERDataset(sentences, labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "sentences, labels = separate_words_tags(testset)\n",
        "test_data = NERDataset(sentences, labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "# Creating DataLoader ; pad_collate function pads sentences to the length of the longest sentence in the batch\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "dev_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87C0jiowmJVE",
        "outputId": "64327a51-d856-4974-ce73-47e278dbeed2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5539 5513\n",
            "3754 3738\n",
            "921 916\n",
            "864 859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5J96jCHip-pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Short Term Memory"
      ],
      "metadata": {
        "id": "poTFRhXbfLKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import string\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqMIgIg1j1g2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amY4ufLSMjD-",
        "outputId": "13c23523-3226-413b-d12f-8e47b8ecdb95"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=87aa98a09eddac6d59637ca0a5ed1cc2b9d85ae240649c7894b94cef65e2c5d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# early stopping obtained from tutorial\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience # how many epochs to wait before stopping when loss is no longer decreasing\n",
        "        self.min_delta = min_delta # minimum difference between new loss and old loss to be considered as a decrease in loss\n",
        "        self.counter = 0 # number of epochs since loss was last decreased\n",
        "        self.min_validation_loss = np.inf # minimum validation loss achieved so far\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss: # new loss is lower than old loss\n",
        "            self.min_validation_loss = validation_loss # update minimum loss\n",
        "            self.counter = 0 # reset counter\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta): # new loss is higher than old loss + minimum difference\n",
        "            self.counter += 1 # increase counter\n",
        "            if self.counter >= self.patience:\n",
        "                return True # stop training\n",
        "        return False # continue training\n",
        "\n",
        "\n",
        "# set random seed\n",
        "def set_seed(seed = 0):\n",
        "    '''\n",
        "    set random seed\n",
        "    '''\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# Train step\n",
        "def train_step(model, trainloader, optimizer, device, lossfn):\n",
        "    model.train()  # set model to training mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for i, data in trainloader:\n",
        "        inputs, labels, _ = data  # get the inputs and labels\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # move them to the device\n",
        "\n",
        "        optimizer.zero_grad()  # zero the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lossfn(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimisation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()  # accumulate the loss\n",
        "        trainloader.set_postfix({'Training loss': '{:.4f}'.format(total_loss/(i+1))})  # Update the progress bar with the training loss\n",
        "\n",
        "    train_loss = total_loss / len(trainloader)\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "\n",
        "# Test step\n",
        "def val_step(model, valloader, lossfn, device):\n",
        "\n",
        "    from seqeval.metrics import f1_score\n",
        "    idx_to_tag = {\n",
        "    1:\"O\",\n",
        "    2: \"B-MISC\",\n",
        "    3: \"B-LOC\",\n",
        "    4: \"I-MISC\",\n",
        "    5: \"I-LOC\",\n",
        "    6: \"B-ORG\",\n",
        "    7: \"I-PER\",\n",
        "    8: \"I-ORG\"\n",
        "}\n",
        "    model.eval() # set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total_words = 0\n",
        "    all_batch_preds = []  # List to store batch predictions\n",
        "    all_batch_labels = []  # List to store batch labels\n",
        "\n",
        "    with torch.no_grad(): # disable gradient calculation\n",
        "        for data in valloader:\n",
        "            inputs, labels, lens = data # get the inputs and labels and actual lengths\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # move them to the device\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = lossfn(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get the index of the max log-probability along the tagset_size dimension\n",
        "            _, predicted = torch.max(outputs.permute(0, 2, 1), 2)\n",
        "\n",
        "            batch_preds = []\n",
        "            batch_labels = []\n",
        "\n",
        "            batch_preds_acc = []\n",
        "            batch_labels_acc = []\n",
        "\n",
        "            for i in range(len(lens)):\n",
        "                batch_preds.append([idx_to_tag[int(word)] for word in predicted[i, :lens[i]]])  # Append predictions but only up to the actual length of the sentence\n",
        "                batch_labels.append([idx_to_tag[int(word)] for word in labels[i, :lens[i]]])\n",
        "\n",
        "                batch_preds_acc.extend(predicted[i, :lens[i]].cpu().numpy())  # Append predictions but only up to the actual length of the sentence\n",
        "                batch_labels_acc.extend(labels[i, :lens[i]].cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "            correct += sum(p == l for p, l in zip(batch_preds_acc, batch_labels_acc))  # Accumulate correct predictions for this batch\n",
        "            total_words += sum(lens)  # Accumulate the actual sentence lengths for this batch\n",
        "\n",
        "            all_batch_preds.extend(batch_preds)  # Append batch predictions to the list\n",
        "            all_batch_labels.extend(batch_labels)  # Append batch labels to the list\n",
        "\n",
        "    val_loss = total_loss / len(valloader)\n",
        "    accuracy = 100 * correct / total_words\n",
        "    f1 = f1_score(all_batch_labels, all_batch_preds,average='macro')\n",
        "\n",
        "    return val_loss, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save model\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train(model, tl, vl, opt, loss, device, epochs, early_stopper, path):\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "    val_acc_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        start_time = time.time()  # Record the start time of the epoch\n",
        "\n",
        "        # Wrap the trainloader with tqdm for the progress bar\n",
        "        pbar = tqdm(enumerate(tl), total=len(tl), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        train_loss = train_step(model, pbar, opt, device, loss)  # Pass the tqdm-wrapped loader\n",
        "        val_loss,val_acc, F1 = val_step(model, vl, loss, device)\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        val_acc_list.append(val_acc)\n",
        "        f1_list.append(F1)\n",
        "\n",
        "        # Print time taken for epoch\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs} took {elapsed_time:.2f}s | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | Val accuracy: {val_acc:.2f}% | F1: {F1:.4f} | EarlyStopper count: {early_stopper.counter}')\n",
        "\n",
        "        # save as last_model after every epoch\n",
        "        save_model(model, os.path.join(path, 'last_model.pt'))\n",
        "\n",
        "        # save as best_model if validation loss is lower than previous best validation loss\n",
        "        if val_loss < early_stopper.min_validation_loss:\n",
        "            save_model(model, os.path.join(path, 'best_model.pt'))\n",
        "\n",
        "        if early_stopper.early_stop(val_loss):\n",
        "            print('Early stopping')\n",
        "            break\n",
        "\n",
        "    return train_loss_list, val_loss_list, val_acc_list, f1_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(w2v.vectors), freeze=False)\n",
        "# word_embeddings(torch.LongTensor([w2v.key_to_index['<UNK>']]))"
      ],
      "metadata": {
        "id": "ty4Sa14eHZqC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Model\n"
      ],
      "metadata": {
        "id": "NSqmeMrfHu63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# LSTM Model\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, weights_matrix=None, freeze_weights=True):\n",
        "        super(NERModel, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        if weights_matrix is not None:\n",
        "            self.word_embeddings = nn.Embedding.from_pretrained(weights_matrix, freeze=freeze_weights) # initialize word embeddings with pretrained weights and freeze them\n",
        "        else:\n",
        "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size) # Linear layer maps from hidden state space to tag space\n",
        "\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        '''\n",
        "        sentences: batch_size x max_seq_length\n",
        "        tag_space: batch_size x tagset_size x max_seq_length\n",
        "        '''\n",
        "        embeds = self.word_embeddings(sentences) # Embed the input sentence\n",
        "\n",
        "        # LSTM input shape: batch_size x max_seq_length x embedding_dim\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "\n",
        "        # LSTM output shape: batch_size x max_seq_length x hidden_dim\n",
        "        # reshape it for the linear layer\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) # shape: (batch_size * max_seq_length) x hidden_dim\n",
        "\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "\n",
        "        # reshape back to batch_size x max_seq_length x tagset_size\n",
        "        tag_space = tag_space.contiguous().view(sentences.shape[0], sentences.shape[1], -1)\n",
        "\n",
        "        # swap dimensions to make it batch_size x tagset_size x max_seq_length\n",
        "        tag_space = tag_space.permute(0, 2, 1)\n",
        "\n",
        "        return tag_space # Return output"
      ],
      "metadata": {
        "id": "9PZCf7IwHwq2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "YFjKEQB5Fa78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "hidden_dim = 16\n",
        "vocab_size = len(word_to_ix)\n",
        "tagset_size = len(tag_to_ix)\n",
        "weights_matrix = torch.FloatTensor(w2v.vectors)\n",
        "\n",
        "\n",
        "model = NERModel(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=vocab_size, tagset_size=tagset_size, weights_matrix=weights_matrix)"
      ],
      "metadata": {
        "id": "81K1SYvRFcE5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss = torch.nn.CrossEntropyLoss(ignore_index=0 )\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 300 # number of epochs\n",
        "early_stopper = EarlyStopper(patience=10) # initialise early stopper\n",
        "\n",
        "# Make directory to save baseline model\n",
        "model_path = \"../saved_models/\"\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# Define the device-specific path\n",
        "device_type = None\n",
        "if device == torch.device(\"cuda\"):\n",
        "    device_type = \"cuda\"\n",
        "elif device == torch.device(\"mps\"):\n",
        "    device_type = \"mps\"\n",
        "else:\n",
        "    device_type = \"cpu\"\n",
        "\n",
        "# Construct the full path\n",
        "device_path = os.path.join(model_path, device_type)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(device_path):\n",
        "    os.mkdir(device_path)"
      ],
      "metadata": {
        "id": "ZcnUbsqKG6mW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, val_loss_list, val_acc_list, f1_list = train(model, train_loader, dev_loader, optimiser, loss, device, epochs, early_stopper, device_path) # train model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "TaB47BRJJsjw",
        "outputId": "b8b3661b-3d32-4e29-8f1f-1273f6dd08a0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/300: 100%|██████████| 937/937 [00:11<00:00, 78.79it/s, Training loss=0.0923]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 took 14.05s | Train loss: 0.0923 | Val loss: 0.1199 | Val accuracy: 96.49% | F1: 0.7789 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/300: 100%|██████████| 937/937 [00:11<00:00, 83.39it/s, Training loss=0.0878]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/300 took 12.43s | Train loss: 0.0878 | Val loss: 0.1195 | Val accuracy: 96.45% | F1: 0.7787 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-7ff5c0a7f8c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-efbcdb7ea579>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, tl, vl, opt, loss, device, epochs, early_stopper, path)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# save as last_model after every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# save as best_model if validation loss is lower than previous best validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-efbcdb7ea579>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, path)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot training and validation loss, and f1 score\n",
        "plt.plot(train_loss_list, label='Training loss')\n",
        "plt.plot(val_loss_list, label='Validation loss')\n",
        "plt.plot(f1_list, label='F1 score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dVS3i_7rJsoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}