{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsCGGYu-JLkt",
        "outputId": "ff57fc1f-241a-44fc-d3a3-4432f5d3dee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhncx65Ido0y",
        "outputId": "e692aeb3-6afe-4ea4-edb7-6548f2a14360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "import os\n",
        "\n",
        "os.chdir(\"..\")"
      ],
      "metadata": {
        "id": "9OlIhoW4vrEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "0UxIrzzxxCu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query the most similar vector\n"
      ],
      "metadata": {
        "id": "Qe580KAqz4f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"student\", \"Apple\", \"apple\"]\n",
        "\n",
        "for w in words:\n",
        "    most_sim_w, score = w2v.most_similar(positive= [w])[0]\n",
        "    print(f\"{w}:  {most_sim_w}, {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8egMwJi7q7V",
        "outputId": "789fa287-cd17-4e70-e613-3ad45bd21c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
            "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "student:  students, 0.729\n",
            "Apple:  Apple_AAPL, 0.746\n",
            "apple:  apples, 0.720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Donwload dataset"
      ],
      "metadata": {
        "id": "2244o3tEF9rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chBpBd-DGA5D",
        "outputId": "b8f11ad5-f2b5-4e49-fa65-e41003a976c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget"
      ],
      "metadata": {
        "id": "Y7NSvzgxHEso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1.2 Download\n",
        "\n",
        "conll_raw_url = \"https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/\"\n",
        "filenames = [\"eng.train\", \"eng.testa\", \"eng.testb\"]\n",
        "\n",
        "urls = {(f, f\"{conll_raw_url}/{f}\") for f in filenames}\n",
        "\n",
        "for fn, url in urls:\n",
        "    save_path = f\"/content/drive/MyDrive/NLPdataset/{fn}\"\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"{fn} already exists. Skipping\")\n",
        "        continue\n",
        "    wget.download(url, save_path)"
      ],
      "metadata": {
        "id": "JYGBOxg_HGTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6fc1c31-6374-4bc1-fd15-a2f85f666044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eng.testa already exists. Skipping\n",
            "eng.testb already exists. Skipping\n",
            "eng.train already exists. Skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition Dataset into training, development and test sets"
      ],
      "metadata": {
        "id": "a75ajFCmeRsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File has one line\n",
        "\n",
        "# First column is the word\n",
        "# Second is POS tag\n",
        "# Third is Consistuency parsing tag\n",
        "# Fourth is NER tag\n",
        "\n",
        "# The NER tagging column\n",
        "\n",
        "\n",
        "root = '/content/drive/MyDrive/NLPdataset/'\n",
        "\n",
        "# Returns a 3 dim array of sentences x words x (word_value, word_category)\n",
        "def process_sets(filepath):\n",
        "    raw = open(filepath)\n",
        "    fin, curr = [], []\n",
        "    tags = []\n",
        "\n",
        "    for r in raw:\n",
        "        if r == \"\\n\":\n",
        "            fin.append(curr)\n",
        "            curr = []\n",
        "            continue\n",
        "        elif 'DOCSTART' in r:# Some files have these which are used to divide sentences\n",
        "          continue\n",
        "        else:\n",
        "          r = r[:-1].split()\n",
        "\n",
        "        if(len(r)!=4):\n",
        "          print(r)\n",
        "        curr.append([r[0],r[3]]) #Extract first and last column (NER Tag)\n",
        "        if r[3] not in tags:\n",
        "          tags.append(r[3])\n",
        "\n",
        "\n",
        "    fin.append(curr)\n",
        "    return fin,tags\n",
        "\n",
        "trainset,traintags = process_sets(root+ \"eng.train\")\n",
        "devset, devtags= process_sets(root +\"eng.testa\")   # aka validation set\n",
        "testset, testtags = process_sets(root +\"eng.testb\")\n",
        "\n",
        "print (f'trainset (No. sentences): {len(trainset)}')\n",
        "print (f'developmentset (No. sentences): {len(devset)}')\n",
        "print (f'testset (No. sentences): {len(testset)}')\n",
        "\n",
        "print(f'tags in training set are {traintags}')\n",
        "print(f'tags in development set are {devtags}')\n",
        "print(f'tags in testing set are {testtags}')\n",
        "print(f'complete set of tags: {set(traintags+devtags+testtags)}')"
      ],
      "metadata": {
        "id": "3qmxfDnyJKrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36448a31-1864-47fb-b5fa-c3b852ace96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainset (No. sentences): 14987\n",
            "developmentset (No. sentences): 3466\n",
            "testset (No. sentences): 3684\n",
            "tags in training set are ['I-ORG', 'O', 'I-MISC', 'I-PER', 'I-LOC', 'B-LOC', 'B-MISC', 'B-ORG']\n",
            "tags in development set are ['O', 'I-ORG', 'I-LOC', 'I-MISC', 'I-PER', 'B-MISC']\n",
            "tags in testing set are ['O', 'I-LOC', 'I-PER', 'I-MISC', 'I-ORG', 'B-ORG', 'B-MISC', 'B-LOC']\n",
            "complete set of tags: {'B-LOC', 'I-ORG', 'B-MISC', 'I-PER', 'B-ORG', 'I-MISC', 'O', 'I-LOC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example sentence"
      ],
      "metadata": {
        "id": "YbqvO-KcfF2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def joinString(entity, joinentity):\n",
        "    if(entity==\"\"):\n",
        "        return joinentity\n",
        "    elif(joinentity[0]==\"'\"):\n",
        "        return entity + joinentity\n",
        "\n",
        "    else:\n",
        "        return entity + \" \" + joinentity\n",
        "\n",
        "\n",
        "def getSentence():\n",
        "    for sentence in trainset:\n",
        "        named_entities = []\n",
        "        previous_word = ''\n",
        "        counter = 0 #Number of entities with more than one word\n",
        "        current_tag = None #B,I,O\n",
        "        entity_tag = None #e.g. PER, LOC, MISC\n",
        "        word_length = 0\n",
        "        for i,word in enumerate(sentence):\n",
        "            if word[1]=='O': #If this is not an entity\n",
        "                current_tag=None\n",
        "                if word_length>1:\n",
        "                    counter+=1\n",
        "                if previous_word!='':\n",
        "                    named_entities.append(previous_word)\n",
        "                previous_word=''\n",
        "                word_length=0\n",
        "            else:\n",
        "                if current_tag is None: #If the current named entity succeeds a 'O' or is the first word of the sentence\n",
        "                    current_tag,entity_tag = word[1].split('-')\n",
        "                    previous_word=joinString(previous_word,word[0])\n",
        "                    word_length=1\n",
        "\n",
        "                else:\n",
        "                    tmp_tag,tmp_type = word[1].split('-')\n",
        "                    if tmp_tag == 'B' or tmp_type!=entity_tag: #Beginning of a new named entity\n",
        "                        if word_length>1:\n",
        "                            counter+=1\n",
        "                        named_entities.append(previous_word)\n",
        "                        previous_word=''\n",
        "                        word_length=0\n",
        "                        current_tag,entitiy_tag=word[1].split('-')\n",
        "                    else: #Continuation of the previous entity\n",
        "                        previous_word=joinString(previous_word,word[0])\n",
        "                        current_tag=tmp_tag\n",
        "                        word_length+=1\n",
        "\n",
        "                    if i == len(sentence)-1 and previous_word!='':\n",
        "                        if(word_length>1):\n",
        "                            counter+=1\n",
        "                        named_entities.append(previous_word)\n",
        "\n",
        "        if counter>2:\n",
        "            return named_entities,sentence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nO-ov1NI2kg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities, sentence_tags = getSentence()\n",
        "\n",
        "sentenceString = \"\"\n",
        "for word in sentence_tags:\n",
        "    sentenceString=joinString(sentenceString,word[0])\n",
        "\n",
        "print(entities)\n",
        "print(sentenceString)\n",
        "print(sentence_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK6zrjsbg_lb",
        "outputId": "3155ad87-4f13-47b1-d2d4-ea4d62346bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Germany', \"Welsh National Farmers' Union\", 'NFU', 'John Lloyd Jones', 'BBC radio']\n",
            "\" What we have to be extremely careful of is how other countries are going to take Germany's lead , \" Welsh National Farmers' Union ( NFU ) chairman John Lloyd Jones said on BBC radio .\n",
            "[['\"', 'O'], ['What', 'O'], ['we', 'O'], ['have', 'O'], ['to', 'O'], ['be', 'O'], ['extremely', 'O'], ['careful', 'O'], ['of', 'O'], ['is', 'O'], ['how', 'O'], ['other', 'O'], ['countries', 'O'], ['are', 'O'], ['going', 'O'], ['to', 'O'], ['take', 'O'], ['Germany', 'I-LOC'], [\"'s\", 'O'], ['lead', 'O'], [',', 'O'], ['\"', 'O'], ['Welsh', 'I-ORG'], ['National', 'I-ORG'], ['Farmers', 'I-ORG'], [\"'\", 'I-ORG'], ['Union', 'I-ORG'], ['(', 'O'], ['NFU', 'I-ORG'], [')', 'O'], ['chairman', 'O'], ['John', 'I-PER'], ['Lloyd', 'I-PER'], ['Jones', 'I-PER'], ['said', 'O'], ['on', 'O'], ['BBC', 'I-ORG'], ['radio', 'I-ORG'], ['.', 'O']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out of vocabulary word preprocessing\n"
      ],
      "metadata": {
        "id": "kmWItuCWYDWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get all words not in the word2vec model\n",
        "def get_words_not_in_model(set, w2v):\n",
        "    abs_w2v = [] # array to store words that are not in the word2vec model\n",
        "    abs_w2v_lower = [] # array to store words that are not in the word2vec model, but are in lower case\n",
        "    for sentences in set:\n",
        "        for word, type in sentences:\n",
        "            if word not in w2v.key_to_index:\n",
        "                #print(word)\n",
        "                abs_w2v.append(word)\n",
        "                if word.lower() not in w2v.key_to_index:\n",
        "                    abs_w2v_lower.append(word.lower())\n",
        "    return abs_w2v, abs_w2v_lower"
      ],
      "metadata": {
        "id": "QH51CeCQbYae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# function to remove words that are puncutation, numbers, or special characters\n",
        "def remove_punc_num_special(s):\n",
        "    clean_set = []\n",
        "    for sentence in s:\n",
        "        clean_sentence = []\n",
        "        for word, tag in sentence:\n",
        "            if any(c.isalpha() for c in word):\n",
        "                clean_sentence.append([word, tag])\n",
        "        clean_set.append(clean_sentence)\n",
        "    return clean_set\n"
      ],
      "metadata": {
        "id": "8_QvwFbfbcr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_stemming(s,w2v):\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_sentences=[]\n",
        "    for sentence in s:\n",
        "        stemmed_sentence=[]\n",
        "        for word, tag in sentence:\n",
        "            if (word not in w2v.key_to_index and  ps.stem(word) in w2v.key_to_index):\n",
        "                stemmed_sentence.append([ps.stem(word),tag])\n",
        "            else:\n",
        "                stemmed_sentence.append([word,tag])\n",
        "        stemmed_sentences.append(stemmed_sentence)\n",
        "    return stemmed_sentences"
      ],
      "metadata": {
        "id": "uynylU3_mn1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(s):\n",
        "    stop_words=['of','a','and','to',\"'s\"]\n",
        "    stopless_sentences=[]\n",
        "    for sentence in s:\n",
        "        stopless_sentence=[]\n",
        "        for word,tag in sentence:\n",
        "            if word not in stop_words:\n",
        "                stopless_sentence.append([word,tag])\n",
        "\n",
        "        stopless_sentences.append(stopless_sentence)\n",
        "    return stopless_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "Dd7zWWCEtNrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyphen_seperation(s,w2v):\n",
        "    seperated_sentences=[]\n",
        "    for sentence in s:\n",
        "        seperated_sentence=[]\n",
        "        for word,tag in sentence:\n",
        "            array_of_words = word.split('-')\n",
        "            for individual_word in array_of_words:\n",
        "                seperated_sentence.append([individual_word,tag])\n",
        "        seperated_sentences.append(seperated_sentence)\n",
        "    return seperated_sentences"
      ],
      "metadata": {
        "id": "_tOrD3Crq60i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check number of words not in word2vec model\n",
        "fullset = trainset + devset + testset\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(fullset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU04NuJbctPA",
        "outputId": "1cf44ca2-6329-45eb-87fc-82e40d363ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83402 83322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map all words to embeddings"
      ],
      "metadata": {
        "id": "HS065tylI-VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "dQI1m9BFOFST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v['<UNK>']=np.zeros(300)\n",
        "w2v['<STOPWORD>'] = np.array([0.1] * 300)\n",
        "w2v['<NONALPHA>'] = np.array([-0.1] * 300)\n",
        "\n",
        "word_to_ix = {\"<PAD>\":3000002,\"<UNK>\":3000002,\"<STOPWORD>\":3000000,\"<NONALPHA>\":3000001}\n",
        "tag_to_ix = {\"<PAD>\": 0, \"O\": 1, \"B-MISC\": 2, \"B-LOC\": 3, \"I-MISC\": 4, \"I-LOC\": 5, \"B-ORG\": 6, \"I-PER\": 7, \"I-ORG\": 8}\n",
        "stop_words=['of','a','and','to',\"'s\"]\n",
        "\n",
        "#Map each word to a metric embedding\n",
        "for sentence in fullset:\n",
        "    for word,tag in sentence:\n",
        "        if word in w2v.key_to_index:\n",
        "            word_to_ix[word]=w2v.key_to_index[word]\n",
        "        elif word in stop_words:\n",
        "            word_to_ix[word]=w2v.key_to_index['<STOPWORD>']\n",
        "        elif not any(char.isalpha() for char in word):\n",
        "             word_to_ix[word]=w2v.key_to_index['<NONALPHA>']\n",
        "        elif word.lower() in w2v.key_to_index:\n",
        "            word_to_ix[word]=w2v.key_to_index[word.lower()]\n",
        "        else:\n",
        "            word_to_ix[word]=w2v.key_to_index['<UNK>']\n"
      ],
      "metadata": {
        "id": "0uPa_Y1wlJBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading"
      ],
      "metadata": {
        "id": "w8K37mrRO4l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import string\n"
      ],
      "metadata": {
        "id": "D-iubbQhO68G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes in a set of sentences in the form of a list of lists of tuples, and\n",
        "# returns a list of list of words and tags\n",
        "def separate_words_tags(set):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for sentence in set:\n",
        "        current_sentence = []\n",
        "        current_label = []\n",
        "        for word, tag in sentence:\n",
        "            current_sentence.append(word)\n",
        "            current_label.append(tag)\n",
        "        sentences.append(current_sentence)\n",
        "        labels.append(current_label)\n",
        "    return sentences, labels\n",
        "\n",
        "# class to create a dataset for the NER task\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.sentences[index]\n",
        "        label = self.labels[index]\n",
        "        original_length = len(sentence)\n",
        "\n",
        "        sentence = [self.word_to_ix.get(word, 3000002) for word in sentence] # 3000002 is the index for <UNK>\n",
        "        label = [self.tag_to_ix[tag] for tag in label]\n",
        "\n",
        "        return torch.tensor(sentence, dtype=torch.long), torch.tensor(label, dtype=torch.long), original_length\n",
        "\n",
        "\n",
        "# function to pad the sequences in a batch\n",
        "def pad_collate(batch):\n",
        "    (xx, yy, lens) = zip(*batch)  # unzip the batch\n",
        "\n",
        "    x_lens = [len(x) for x in xx]  # get lengths of sequences\n",
        "    y_lens = [len(y) for y in yy]  # get lengths of labels\n",
        "\n",
        "    max_x_len = max(x_lens)\n",
        "    max_y_len = max(y_lens)\n",
        "\n",
        "    xx_pad = torch.full((len(xx), max_x_len), 3000002, dtype=torch.long)  # create a matrix filled with 3000002\n",
        "    yy_pad = torch.zeros(len(yy), max_y_len, dtype=torch.long)  # create a matrix of zeros with correct dimensions\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(xx, yy)):\n",
        "        xx_pad[i, :x_lens[i]] = x\n",
        "        yy_pad[i, :y_lens[i]] = y\n",
        "\n",
        "    return xx_pad, yy_pad, lens\n"
      ],
      "metadata": {
        "id": "1XuOB4rLEAyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean dataset by removing meaningless words\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "#fullset = remove_punc_num_special(fullset) #Remove punctuation, special characters and numbers\n",
        "fullset = hyphen_seperation(fullset,w2v) #Address hyphen separated words\n",
        "fullset = word_stemming(fullset,w2v) #Porter Stemmer\n",
        "#fullset = remove_stop_words(fullset) #Remove stop words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(fullset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "#Trainset Transformation\n",
        "#trainset = remove_punc_num_special(trainset) #Remove punctuation, special characters and numbers\n",
        "trainset = hyphen_seperation(trainset,w2v) #Address hyphen separated words\n",
        "trainset = word_stemming(trainset,w2v) #Porter Stemmer\n",
        "#trainset = remove_stop_words(trainset) #Remove stop words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(trainset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "#Validation set transformation\n",
        "#devset = remove_punc_num_special(devset) #Remove punctuation, special characters and numbers\n",
        "devset = hyphen_seperation(devset,w2v) #Address hyphen separated words\n",
        "devset = word_stemming(devset,w2v) #Porter Stemmer\n",
        "#devset = remove_stop_words(devset) #Remove stop words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(devset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "#Test set transformation\n",
        "#testset = remove_punc_num_special(testset) #Remove punctuation, special characters and numbers\n",
        "testset = hyphen_seperation(testset,w2v) #Address hyphen separated words\n",
        "testset = word_stemming(testset,w2v) #Porter Stemmer\n",
        "#testset = remove_stop_words(testset) #Remove stop words\n",
        "abs_w2v, abs_w2v_lower = get_words_not_in_model(testset, w2v)\n",
        "print(len(abs_w2v), len(abs_w2v_lower))\n",
        "\n",
        "\n",
        "#train\n",
        "\n",
        "sentences, labels = separate_words_tags(trainset) # separate words and tags\n",
        "train_data = NERDataset(sentences, labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "sentences, labels = separate_words_tags(devset)\n",
        "dev_data = NERDataset(sentences, labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "sentences, labels = separate_words_tags(testset)\n",
        "test_data = NERDataset(sentences, labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "# Creating DataLoader ; pad_collate function pads sentences to the length of the longest sentence in the batch\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "dev_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87C0jiowmJVE",
        "outputId": "ec71150e-12ac-4540-8f89-22773e58a642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86142 86116\n",
            "57570 57554\n",
            "14228 14223\n",
            "14344 14339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5J96jCHip-pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Short Term Memory"
      ],
      "metadata": {
        "id": "poTFRhXbfLKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import string\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqMIgIg1j1g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amY4ufLSMjD-",
        "outputId": "1c599313-827b-4678-a6e8-29a6407fe5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# early stopping obtained from tutorial\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience # how many epochs to wait before stopping when loss is no longer decreasing\n",
        "        self.min_delta = min_delta # minimum difference between new loss and old loss to be considered as a decrease in loss\n",
        "        self.counter = 0 # number of epochs since loss was last decreased\n",
        "        self.min_validation_loss = np.inf # minimum validation loss achieved so far\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss: # new loss is lower than old loss\n",
        "            self.min_validation_loss = validation_loss # update minimum loss\n",
        "            self.counter = 0 # reset counter\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta): # new loss is higher than old loss + minimum difference\n",
        "            self.counter += 1 # increase counter\n",
        "            if self.counter >= self.patience:\n",
        "                return True # stop training\n",
        "        return False # continue training\n",
        "\n",
        "\n",
        "# set random seed\n",
        "def set_seed(seed = 0):\n",
        "    '''\n",
        "    set random seed\n",
        "    '''\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# Train step\n",
        "def train_step(model, trainloader, optimizer, device, lossfn):\n",
        "    model.train()  # set model to training mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for i, data in trainloader:\n",
        "        inputs, labels, _ = data  # get the inputs and labels\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # move them to the device\n",
        "\n",
        "        optimizer.zero_grad()  # zero the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lossfn(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimisation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()  # accumulate the loss\n",
        "        trainloader.set_postfix({'Training loss': '{:.4f}'.format(total_loss/(i+1))})  # Update the progress bar with the training loss\n",
        "\n",
        "    train_loss = total_loss / len(trainloader)\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "\n",
        "# Test step\n",
        "def val_step(model, valloader, lossfn, device):\n",
        "\n",
        "    from seqeval.metrics import f1_score\n",
        "    idx_to_tag = {\n",
        "    1:\"O\",\n",
        "    2: \"B-MISC\",\n",
        "    3: \"B-LOC\",\n",
        "    4: \"I-MISC\",\n",
        "    5: \"I-LOC\",\n",
        "    6: \"B-ORG\",\n",
        "    7: \"I-PER\",\n",
        "    8: \"I-ORG\"\n",
        "}\n",
        "    model.eval() # set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total_words = 0\n",
        "    all_batch_preds = []  # List to store batch predictions\n",
        "    all_batch_labels = []  # List to store batch labels\n",
        "\n",
        "    with torch.no_grad(): # disable gradient calculation\n",
        "        for data in valloader:\n",
        "            inputs, labels, lens = data # get the inputs and labels and actual lengths\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # move them to the device\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = lossfn(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get the index of the max log-probability along the tagset_size dimension\n",
        "            _, predicted = torch.max(outputs.permute(0, 2, 1), 2)\n",
        "\n",
        "            batch_preds = []\n",
        "            batch_labels = []\n",
        "\n",
        "            batch_preds_acc = []\n",
        "            batch_labels_acc = []\n",
        "\n",
        "            for i in range(len(lens)):\n",
        "                batch_preds.append([idx_to_tag[int(word)] for word in predicted[i, :lens[i]]])  # Append predictions but only up to the actual length of the sentence\n",
        "                batch_labels.append([idx_to_tag[int(word)] for word in labels[i, :lens[i]]])\n",
        "\n",
        "                batch_preds_acc.extend(predicted[i, :lens[i]].cpu().numpy())  # Append predictions but only up to the actual length of the sentence\n",
        "                batch_labels_acc.extend(labels[i, :lens[i]].cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "            correct += sum(p == l for p, l in zip(batch_preds_acc, batch_labels_acc))  # Accumulate correct predictions for this batch\n",
        "            total_words += sum(lens)  # Accumulate the actual sentence lengths for this batch\n",
        "\n",
        "            all_batch_preds.extend(batch_preds)  # Append batch predictions to the list\n",
        "            all_batch_labels.extend(batch_labels)  # Append batch labels to the list\n",
        "\n",
        "    val_loss = total_loss / len(valloader)\n",
        "    accuracy = 100 * correct / total_words\n",
        "    f1 = f1_score(all_batch_labels, all_batch_preds,average='macro')\n",
        "\n",
        "    return val_loss, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save model\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train(model, tl, vl, opt, loss, device, epochs, early_stopper, path):\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "    val_acc_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        start_time = time.time()  # Record the start time of the epoch\n",
        "\n",
        "        # Wrap the trainloader with tqdm for the progress bar\n",
        "        pbar = tqdm(enumerate(tl), total=len(tl), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        train_loss = train_step(model, pbar, opt, device, loss)  # Pass the tqdm-wrapped loader\n",
        "        val_loss,val_acc, F1 = val_step(model, vl, loss, device)\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        val_acc_list.append(val_acc)\n",
        "        f1_list.append(F1)\n",
        "\n",
        "        # Print time taken for epoch\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs} took {elapsed_time:.2f}s | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | Val accuracy: {val_acc:.2f}% | F1: {F1:.4f} | EarlyStopper count: {early_stopper.counter}')\n",
        "\n",
        "        # save as last_model after every epoch\n",
        "        save_model(model, os.path.join(path, 'last_model.pt'))\n",
        "\n",
        "        # save as best_model if validation loss is lower than previous best validation loss\n",
        "        if val_loss < early_stopper.min_validation_loss:\n",
        "            save_model(model, os.path.join(path, 'best_model.pt'))\n",
        "\n",
        "        if early_stopper.early_stop(val_loss):\n",
        "            print('Early stopping')\n",
        "            break\n",
        "\n",
        "    return train_loss_list, val_loss_list, val_acc_list, f1_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(w2v.vectors), freeze=False)\n",
        "# word_embeddings(torch.LongTensor([w2v.key_to_index['<UNK>']]))"
      ],
      "metadata": {
        "id": "ty4Sa14eHZqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Model\n"
      ],
      "metadata": {
        "id": "NSqmeMrfHu63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# LSTM Model\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, weights_matrix=None, freeze_weights=True):\n",
        "        super(NERModel, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        if weights_matrix is not None:\n",
        "            self.word_embeddings = nn.Embedding.from_pretrained(weights_matrix, freeze=freeze_weights) # initialize word embeddings with pretrained weights and freeze them\n",
        "        else:\n",
        "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,bidirectional=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size) # Linear layer maps from hidden state space to tag space\n",
        "\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        '''\n",
        "        sentences: batch_size x max_seq_length\n",
        "        tag_space: batch_size x tagset_size x max_seq_length\n",
        "        '''\n",
        "        embeds = self.word_embeddings(sentences) # Embed the input sentence\n",
        "\n",
        "        # LSTM input shape: batch_size x max_seq_length x embedding_dim\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "\n",
        "        # LSTM output shape: batch_size x max_seq_length x hidden_dim\n",
        "        # reshape it for the linear layer\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) # shape: (batch_size * max_seq_length) x hidden_dim\n",
        "\n",
        "        lstm_out=self.dropout(lstm_out)\n",
        "\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "\n",
        "        # reshape back to batch_size x max_seq_length x tagset_size\n",
        "        tag_space = tag_space.contiguous().view(sentences.shape[0], sentences.shape[1], -1)\n",
        "\n",
        "        # swap dimensions to make it batch_size x tagset_size x max_seq_length\n",
        "        tag_space = tag_space.permute(0, 2, 1)\n",
        "\n",
        "        return tag_space # Return output"
      ],
      "metadata": {
        "id": "9PZCf7IwHwq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "YFjKEQB5Fa78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "hidden_dim = 512\n",
        "vocab_size = len(word_to_ix)\n",
        "tagset_size = len(tag_to_ix)\n",
        "weights_matrix = torch.FloatTensor(w2v.vectors)\n",
        "\n",
        "\n",
        "model = NERModel(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=vocab_size, tagset_size=tagset_size, weights_matrix=weights_matrix)"
      ],
      "metadata": {
        "id": "81K1SYvRFcE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss = torch.nn.CrossEntropyLoss(ignore_index=0 )\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 300 # number of epochs\n",
        "early_stopper = EarlyStopper(patience=2) # initialise early stopper\n",
        "\n",
        "# Make directory to save baseline model\n",
        "model_path = \"../saved_models/\"\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# Define the device-specific path\n",
        "device_type = None\n",
        "if device == torch.device(\"cuda\"):\n",
        "    device_type = \"cuda\"\n",
        "elif device == torch.device(\"mps\"):\n",
        "    device_type = \"mps\"\n",
        "else:\n",
        "    device_type = \"cpu\"\n",
        "\n",
        "# Construct the full path\n",
        "device_path = os.path.join(model_path, device_type)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(device_path):\n",
        "    os.mkdir(device_path)"
      ],
      "metadata": {
        "id": "ZcnUbsqKG6mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, val_loss_list, val_acc_list, f1_list = train(model, train_loader, dev_loader, optimiser, loss, device, epochs, early_stopper, device_path) # train model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaB47BRJJsjw",
        "outputId": "3712f5c0-3ce5-4892-f138-122c7030c2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/300: 100%|██████████| 937/937 [00:06<00:00, 150.49it/s, Training loss=0.2544]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 took 8.73s | Train loss: 0.2544 | Val loss: 0.1199 | Val accuracy: 96.36% | F1: 0.7319 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/300: 100%|██████████| 937/937 [00:06<00:00, 146.88it/s, Training loss=0.1133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/300 took 8.90s | Train loss: 0.1133 | Val loss: 0.1021 | Val accuracy: 97.06% | F1: 0.7824 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/300: 100%|██████████| 937/937 [00:06<00:00, 146.69it/s, Training loss=0.0950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/300 took 8.91s | Train loss: 0.0950 | Val loss: 0.0906 | Val accuracy: 97.27% | F1: 0.7865 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/300: 100%|██████████| 937/937 [00:06<00:00, 147.20it/s, Training loss=0.0818]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/300 took 8.93s | Train loss: 0.0818 | Val loss: 0.0904 | Val accuracy: 97.25% | F1: 0.7939 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/300: 100%|██████████| 937/937 [00:06<00:00, 148.24it/s, Training loss=0.0722]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/300 took 8.87s | Train loss: 0.0722 | Val loss: 0.0823 | Val accuracy: 97.56% | F1: 0.8147 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/300: 100%|██████████| 937/937 [00:06<00:00, 151.14it/s, Training loss=0.0635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/300 took 8.78s | Train loss: 0.0635 | Val loss: 0.0815 | Val accuracy: 97.65% | F1: 0.8211 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/300: 100%|██████████| 937/937 [00:06<00:00, 152.19it/s, Training loss=0.0555]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/300 took 8.71s | Train loss: 0.0555 | Val loss: 0.0865 | Val accuracy: 97.50% | F1: 0.8119 | EarlyStopper count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/300: 100%|██████████| 937/937 [00:06<00:00, 146.48it/s, Training loss=0.0496]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/300 took 8.96s | Train loss: 0.0496 | Val loss: 0.0878 | Val accuracy: 97.49% | F1: 0.8151 | EarlyStopper count: 1\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot training and validation loss, and f1 score\n",
        "plt.plot(train_loss_list, label='Training loss')\n",
        "plt.plot(val_loss_list, label='Validation loss')\n",
        "plt.plot(f1_list, label='F1 score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "dVS3i_7rJsoT",
        "outputId": "f327ccf1-a780-4d7c-ecc4-466640030f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test step\n",
        "\n",
        "\n",
        "from seqeval.metrics import f1_score\n",
        "idx_to_tag = {\n",
        "1:\"O\",\n",
        "2: \"B-MISC\",\n",
        "3: \"B-LOC\",\n",
        "4: \"I-MISC\",\n",
        "5: \"I-LOC\",\n",
        "6: \"B-ORG\",\n",
        "7: \"I-PER\",\n",
        "8: \"I-ORG\"\n",
        "}\n",
        "model.eval() # set model to evaluation mode\n",
        "correct = 0\n",
        "total_words = 0\n",
        "all_batch_preds = []  # List to store batch predictions\n",
        "all_batch_labels = []  # List to store batch labels\n",
        "\n",
        "with torch.no_grad(): # disable gradient calculation\n",
        "    for data in test_loader:\n",
        "        inputs, labels, lens = data # get the inputs and labels and actual lengths\n",
        "        inputs, labels = inputs.to(device), labels.to(device) # move them to the device\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Get the index of the max log-probability along the tagset_size dimension\n",
        "        _, predicted = torch.max(outputs.permute(0, 2, 1), 2)\n",
        "\n",
        "        batch_preds = []\n",
        "        batch_labels = []\n",
        "\n",
        "        batch_preds_acc = []\n",
        "        batch_labels_acc = []\n",
        "\n",
        "        for i in range(len(lens)):\n",
        "            batch_preds.append([idx_to_tag[int(word)] for word in predicted[i, :lens[i]]])  # Append predictions but only up to the actual length of the sentence\n",
        "            batch_labels.append([idx_to_tag[int(word)] for word in labels[i, :lens[i]]])\n",
        "\n",
        "            batch_preds_acc.extend(predicted[i, :lens[i]].cpu().numpy())  # Append predictions but only up to the actual length of the sentence\n",
        "            batch_labels_acc.extend(labels[i, :lens[i]].cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "        correct += sum(p == l for p, l in zip(batch_preds_acc, batch_labels_acc))  # Accumulate correct predictions for this batch\n",
        "        total_words += sum(lens)  # Accumulate the actual sentence lengths for this batch\n",
        "\n",
        "        all_batch_preds.extend(batch_preds)  # Append batch predictions to the list\n",
        "        all_batch_labels.extend(batch_labels)  # Append batch labels to the list\n",
        "\n",
        "accuracy = 100 * correct / total_words\n",
        "f1 = f1_score(all_batch_labels, all_batch_preds,average='macro')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PrswPMJts89a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy: {accuracy: .2f}%, F1 Score: {f1: .2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YsvMrlSt_N8",
        "outputId": "34c7ac46-74cc-48e9-919f-c890f8246892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  96.25%, F1 Score:  0.74\n"
          ]
        }
      ]
    }
  ]
}