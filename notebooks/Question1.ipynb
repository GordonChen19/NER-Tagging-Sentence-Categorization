{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dd14f5-8dbd-4551-9dc2-b9d2cf537a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import gensim.downloader\n",
    "import os\n",
    "import wget\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee799866-2708-4acf-a083-920c0f353ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.1 Download\n",
    "\n",
    "# This downloads under the directory \"~/gensim-data\" by default\n",
    "# change GENSIM_DATA_DIR environment variable if you don't want this\n",
    "# size is about 1.6 GB\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15af645c-20b5-48fc-8e90-48fef90c71de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, gensim.models.keyedvectors.KeyedVectors)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 million words\n",
    "len(w2v), type(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58db361e-beb9-49bc-a014-da87ba7cfc07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22753906 -0.34570312  0.0625      0.11132812  0.17089844  0.03442383\n",
      "  0.13574219  0.16699219  0.07177734 -0.07421875]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each vector is 300 long\n",
    "print(w2v[\"compute\"][:10])\n",
    "w2v[\"compute\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f18d773-b893-4127-8785-30591af1785d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Question 1.2 Download\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "conll_raw_url = \"https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/\"\n",
    "filenames = [\"eng.train\", \"eng.testa\", \"eng.testb\"] \n",
    "\n",
    "urls = {(f, f\"{conll_raw_url}/{f}\") for f in filenames}\n",
    "\n",
    "for fn, url in urls:\n",
    "    save_path = f\"data/{fn}\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"{fn} already exists. Skipping\")\n",
    "        continue\n",
    "    wget.download(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a14467-8e9f-44bb-a40b-bb99548dfb92",
   "metadata": {},
   "source": [
    "# Question 1. 1\n",
    "\n",
    "The most similar words are\n",
    "\n",
    "    (a) student:  students, 0.729\n",
    "    (b) Apple:  Apple_AAPL, 0.746\n",
    "    (c) apple:  apples, 0.720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05be0f8f-b86b-4b3f-8662-53857c88d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student:  students, 0.729\n",
      "Apple:  Apple_AAPL, 0.746\n",
      "apple:  apples, 0.720\n"
     ]
    }
   ],
   "source": [
    "# (a) “student”; (b) “Apple”; (c) “apple”\n",
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "\n",
    "for w in words:\n",
    "    most_sim_w, score = w2v.most_similar(w)[0]\n",
    "    print(f\"{w}:  {most_sim_w}, {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71561fb2-9e4f-4fef-83e4-582c6b20dce6",
   "metadata": {},
   "source": [
    "# Question 1.2\n",
    "\n",
    "    (a1) training set has 14987 sentences\n",
    "    (a2) development set has 3466 sentences\n",
    "    (a3) testing set has 3684 sentences\n",
    "    (a4) train tags      : {'O', 'B-MISC', 'B-LOC', 'I-MISC', 'I-LOC', 'B-ORG', 'I-PER', 'I-ORG'}\n",
    "    (a5) development tags: {'O', 'B-MISC', 'I-MISC', 'I-LOC', 'I-PER', 'I-ORG'}\n",
    "    (a6) testing tags    : {'O', 'B-MISC', 'B-LOC', 'I-MISC', 'I-LOC', 'B-ORG', 'I-PER', 'I-ORG'}\n",
    "\n",
    "    (b1) 9/16 - Luo Yigang (China) beat Hwang Sun-ho (South Korea) 15-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b507a9a-8536-4b3d-8c76-822100bf278b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14987, 3466, 3684)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File has one line\n",
    "\n",
    "# First column is the word\n",
    "# Second is POS tag\n",
    "# Third is Consistuency parsing tag\n",
    "# Fourth is NER tag\n",
    "\n",
    "# The NER tagging column\n",
    "TAGGING_INDEX  = 3\n",
    "\n",
    "# Returns a 3 dim array of sentences x words x (word_value, word_category)\n",
    "def process_sets(filepath):\n",
    "    raw = open(filepath)\n",
    "    fin, curr = [], []\n",
    "    \n",
    "    for r in raw:\n",
    "        if r == \"\\n\":\n",
    "            fin.append(curr)\n",
    "            curr = []\n",
    "            continue\n",
    "            \n",
    "        r = r[:-1].split()\n",
    "\n",
    "        # Some files have these which are used to divide sentences\n",
    "        if r[0] == \"-DOCSTART-\":\n",
    "            continue\n",
    "        \n",
    "        r = [r[i] for i in (0, TAGGING_INDEX)]  # select first and last columns\n",
    "        curr.append(r)\n",
    "    \n",
    "    fin.append(curr)\n",
    "    return fin\n",
    "\n",
    "trainset = process_sets(\"data/eng.train\")\n",
    "devset = process_sets(\"data/eng.testa\")   # aka validation set\n",
    "testset = process_sets(\"data/eng.testb\")\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83c5d40-f598-4461-9ae2-5b7957f7f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tags: {'O', 'I-PER', 'I-LOC', 'B-LOC', 'B-MISC', 'I-ORG', 'I-MISC', 'B-ORG'}\n",
      "development tags: {'I-PER', 'O', 'I-LOC', 'B-MISC', 'I-ORG', 'I-MISC'}\n",
      "testing tags: {'O', 'I-PER', 'I-LOC', 'B-LOC', 'B-MISC', 'I-ORG', 'I-MISC', 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "train_tags = set(w[1] for s in trainset for w in s)\n",
    "dev_tags = set(w[1] for s in devset for w in s)\n",
    "test_tags = set(w[1] for s in testset for w in s)\n",
    "\n",
    "print(f\"train tags: {train_tags}\")\n",
    "print(f\"development tags: {dev_tags}\")\n",
    "print(f\"testing tags: {test_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0685caea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'I-ORG'],\n",
       " ['rejects', 'O'],\n",
       " ['German', 'I-MISC'],\n",
       " ['call', 'O'],\n",
       " ['to', 'O'],\n",
       " ['boycott', 'O'],\n",
       " ['British', 'I-MISC'],\n",
       " ['lamb', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d384a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_eg = [] # array to store sentences with 2 consecutive tags\n",
    "\n",
    "for sentence in trainset:\n",
    "    count = 0 # count of consecutive types\n",
    "    count_buffer = 0 # buffer to store running count of identical types\n",
    "    current_tag = None \n",
    "\n",
    "    for set in sentence: # iterate through each word in sentence \n",
    "        if set[1] == 'O': # if word is not a named entity\n",
    "            current_tag = None\n",
    "            continue\n",
    "\n",
    "        elif current_tag == None: # if this is the first named entity after a non-named entity or start of sentence\n",
    "            current_tag = set[1]\n",
    "\n",
    "        elif (set[1][2:] == current_tag[2:]) and count_buffer == 0: # if this is the first word that follows the same entity as the previous word\n",
    "            count += 1\n",
    "            count_buffer += 1\n",
    "\n",
    "        elif set[1][2:] != current_tag[2:]: # if this is a different entity from the previous word\n",
    "            current_tag = set[1]\n",
    "            count_buffer = 0\n",
    "\n",
    "        else: # if this follows the same entity as the previous word but is not the first word to do so\n",
    "            continue\n",
    "\n",
    "    if count == 2: # if there are 2 consecutive named entities, add the sentence to the array\n",
    "        sent_eg.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abbfb246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['9/16', 'O'], ['-', 'O'], ['Luo', 'I-PER'], ['Yigang', 'I-PER'], ['(', 'O'], ['China', 'I-LOC'], [')', 'O'], ['beat', 'O'], ['Hwang', 'I-PER'], ['Sun-ho', 'I-MISC'], ['(', 'O'], ['South', 'I-LOC'], ['Korea', 'I-LOC'], [')', 'O'], ['15-3', 'O']]\n",
      "['9/16', '-', 'Luo', 'Yigang', '(', 'China', ')', 'beat', 'Hwang', 'Sun-ho', '(', 'South', 'Korea', ')', '15-3']\n"
     ]
    }
   ],
   "source": [
    "# get random sentence with 2 consecutive named entities\n",
    "print(sent_eg[0])\n",
    "# print just the words \n",
    "print([word[0] for word in sent_eg[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
